<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lin Zhao</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Lin Zhao
                </p>
                <p>I am a second-year Ph.D. student in <a href="https://coe.northeastern.edu/">College of Engineering, Northeastern University</a>, supervised by <a href="https://coe.northeastern.edu/people/wang-yanzhi/">Prof. Yanzhi Wang</a> and <a href="https://coe.northeastern.edu/people/lin-xue/">Prof. Xue Lin</a>. My research interests lie in efficient AI and generative model. Currently, I focus on improving the efficiency and quality of video generation models. I got my M.S. and B.S. degrees from <a href="https://www.nankai.edu.cn/">Nankai University</a></a>.
                </p>
                </p>
                <p><strong>Open to collaboration, curious conversations, or just making new friends — feel free to say hi!</strong></a>
                </p>
                <p style="text-align:center">
                  <a href="mailto:zhao.lin1@northeastern.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=Rw9iX_oAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/LinZhao0309">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/lin-zhao-431808352/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="images/wechat.jpg">Wechat</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/LinZhao.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 0%;" alt="profile photo" src="images/LinZhao.jpg" class="hoverZoomLink"></a>
                <p style="font-size:0.85em; text-align:center; color:gray; margin-top:8px;">
                  Photo by <a href="https://oshikaka.github.io/">Xinru Jiang</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/s2dit.png" alt="clean-usnob" width="320" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2601.12719">
                  <span class="papertitle">S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation</span>
                </a>
                <br>
                <strong>Lin Zhao*</strong>, <a href="">Yushu Wu*</a>, <a href="">Aleksei Lebedev</a>, <a href="">Dishani Lahiri</a>, <a href="">Meng Dong</a>, <a href="">Arpit Sahni</a>, <a href="">Michael Vasilkovsky</a>, <a href="">Hao Chen</a>, <a href="">Ju Hu</a>, <a href="">Aliaksandr Siarohin</a>, <a href="">Sergey Tulyakov</a>, <a href="">Yanzhi Wang</a>, <a href="">Anil Kag</a>, <a href="">Yanyu Li</a>
                <br>
                <em>Preprint</em>
                <p>We propose S²DiT, a Sandwich diffusion transformer for mobile Streaming video generation. <a href="https://arxiv.org/abs/2601.12719">[paper]</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/HierAmp.jpg" alt="clean-usnob" width="320" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="">
                  <span class="papertitle">HierAmp: Coarse-to-Fine Autoregressive Amplification for Generative Dataset Distillation</span>
                </a>
                <br>
                <strong>Lin Zhao*</strong>, <a href="">Xinru Jiang*</a>, <a href="">Xi Xiao</a>, <a href="">Qihui Fan</a>, <a href="">Lei Lu</a>, <a href="">Yanzhi Wang</a>, <a href="">Xue Lin</a>, <a href="">Octavia Camps</a>, <a href="">Pu Zhao</a>, <a href="">Jianyang Gu</a>
                <br>
                <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2026
                <p>We propose HierAmp, which enhances dataset distillation by amplifying hierarchical semantics across Vision Autoregressive scales to improve discriminative structural representation. <a href="">[paper] <a href="">[code]</a></p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/survey.jpg" alt="clean-usnob" width="320" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2510.13219">
                  <span class="papertitle">Prompt-based Adaptation in Large-scale Vision Models: A Survey</span>
                </a>
                <br>
                <a href="">Xi Xiao*</a>, <a href="">Yunbei Zhang*</a>, <strong>Lin Zhao*</strong>, <a href="">Yiyang Liu*</a>, <a href="">Xiaoying Liao</a>, <a href="">Zheda Mai</a>, <a href="">Xingjian Li</a>, <a href="">Xiao Wang</a>, <a href="">Hao Xu</a>, <a href="">Jihun Hamm</a>, <a href="">Xue Lin</a>, <a href="">Min Xu</a>, <a href="">Qifan Wang</a>, <a href="">Tianyang Wang</a>, <a href="">Cheng Han</a>
                <br>
                <em>Transactions on Machine Learning Research (<strong>TMLR</strong>)</em>, 2026
                <p>We provide a systematic review and categorization of recent PA algorithms and their practical implementations. <a href="https://arxiv.org/abs/2510.13219">[paper] <a href="https://github.com/yunbeizhang/Awesome-Visual-Prompt-Tuning">[code]</a></p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/D3HR.png" alt="clean-usnob" width="320" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.arxiv.org/abs/2505.18399">
                  <span class="papertitle">Taming Diffusion for Dataset Distillation with High Representativeness</span>
                </a>
                <br>
                <strong>Lin Zhao</strong>, <a href="">Yushu Wu</a>, <a href="">Xinru Jiang</a>, <a href="">Jianyang Gu</a>, <a href="">Yanzhi Wang</a>, <a href="">Xiaolin Xu</a>, <a href="">Pu Zhao</a>, <a href="">Xue Lin</a>
                <br>
                <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2025
                <p>We propose D3HR, which systematically investigates key issues in current diffusion-based methods and, based on these insights, introduces a new paradigm for diffusion-based dataset distillation. <a href="https://www.arxiv.org/abs/2505.18399">[paper] <a href="https://github.com/lin-zhao-resoLve/D3HR">[code]</a></p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/FlashEval.png" alt="clean-usnob" width="320" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_FlashEval_Towards_Fast_and_Accurate_Evaluation_of_Text-to-image_Diffusion_Generative_CVPR_2024_paper.pdf">
                  <span class="papertitle">FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models</span>
                </a>
                <br>
                <strong>Lin Zhao*</strong>, <a href="">Tianchen Zhao*</a>, <a href="">Zinan Lin</a>, <a href="">Xuefei Ning</a>, <a href="">Guohao Dai</a>, <a href="">Huazhong Yang</a>, <a href="">Yu Wang</a>
                <br>
                <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
                <p>We propose FlashEval, which can identify a representative subset to speed up the evaluation of text-to-image Diffusion models (10x). <a href="http://a-suozhang.xyz/flasheval.github.io/">[project] <a href="https://arxiv.org/abs/2403.16379">[paper] <a href="https://github.com/thu-nics/FlashEval">[code]</a></p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/THInImg.png" alt="clean-usnob" width="320" height="150">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_THInImg_Cross-Modal_Steganography_for_Presenting_Talking_Heads_in_Images_WACV_2024_paper.pdf">
                  <span class="papertitle">THInImg: Cross-modal Steganography for Presenting Talking Heads in Images</span>
                </a>
                <br>
                <strong>Lin Zhao</strong>, <a href="">Hongxuan Li</a>, <a href="">Xuefei Ning</a>, <a href="">Xinru Jiang</a>
                <br>
                <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2024
                <p>We propose THInImg, which can present up to 80 seconds of high quality talking-head video (including audio) in an identity image with 160×160 resolution. <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Zhao_THInImg_Cross-Modal_Steganography_for_Presenting_Talking_Heads_in_Images_WACV_2024_paper.pdf">[paper]</a></p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Enhancement.png" alt="clean-usnob" width="320" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Deep_Symmetric_Network_for_Underexposed_Image_Enhancement_With_Recurrent_Attentional_ICCV_2021_paper.pdf">
                  <span class="papertitle">Deep Symmetric Network for Underexposed Image Enhancement with Recurrent Attentional Learning</span>
                </a>
                <br>
                <strong>Lin Zhao*</strong>, <a href="">Shaoping Lu*</a>, <a href="">Tao Chen</a>, <a href="">Zhenglu Yang</a>, <a href="">Ariel Shamir</a>
                <br>
                <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                <p>We propose an invertible framework to solve both underexposed image enhancement and low-light image enhancement problems in a unified structure. <a href="https://www.shaopinglu.net/proj-iccv21/ImageEnhancement.html">[project] <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Deep_Symmetric_Network_for_Underexposed_Image_Enhancement_With_Recurrent_Attentional_ICCV_2021_paper.pdf">[paper] <a href="https://github.com/lin-zhao-resoLve/Deep-Symmetric-Network-Enhancement">[code]</a></p>
              </td>
            </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Work Experiences</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/snap.jpg" width="120" height="120"></td>
              <td width="75%" valign="center">
                <p><strong>Research Intern, Creative Vision Group, Snap Inc.</strong></p> 
                <em>Jun 2025 - Nov 2025, Santa Monica, CA</em>
                <p>Worked on Efficient Text-to-video Diffusion</p> 
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Infinigence.jpg" width="200" height="100"></td>
              <td width="75%" valign="center">
                <p><strong>Research Intern, Infinigence AI</strong></p> 
                <em>Jul 2023 - Dec 2023, Beijing, China</em>
                <p>Worked on Efficient Diffusion Evaluation</p> 
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/msra.png" width="200" height="100"></td>
              <td width="75%" valign="center">
                <p><strong>Research Intern, Media Computing Group, Microsoft Research Asia</strong></p> 
                <em>Jul 2022 - Dec 2022, Beijing, China</em>
                <p>Worked on Talking-head Generation and Video Compression</p> 
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/sensetime.png" width="200" height="80"></td>
              <td width="75%" valign="center">
                <p><strong>Research Intern, Multimodal Group, Sensetime</strong></p> 
                <em>Mar 2022 - May 2022, Beijing, China</em>
                <p>Worked on Video Editing</p> 
              </td>
            </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Selected Honors & Awards</h2>
                  <p>Gongneng Scholarship. 2021</p> 
                  <p>Graduate Student Scholarship. 2020</p>
                  <p>Comprehensive First-class Scholarship (Top 5%). 2018, 2019</p>
                  <p>Honorable Mention, Mathematical Contest in Modeling. 2019</p>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>



            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Misc</h2> 
                  <p>I was born and raised in <a href="https://en.wikipedia.org/wiki/Harbin">Harbin, China</a>.</p> 
                  <p><a href="images/cat.jpg">My baby cat! She is so cute!! </a></p>
                  <p>I especially love little monkeys; they are just too adorable!</p>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            </tbody>
            <a href="https://clustrmaps.com/site/1c1vt"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=XX51c4aOOh2OQm10KNfLEwc4iyOnS0YF5455gNsRRK4&cl=ffffff" /></a>
            </tbody>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
